{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all' # to print multiple outputs from the same cell\n",
    "import math\n",
    "import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from operator import index\n",
    "from collections import defaultdict\n",
    "from scipy.stats import pearsonr\n",
    "from datetime import datetime\n",
    "#from lingua import Language, LanguageDetectorBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "users_df = pd.read_csv(\"dataset/users.csv\")\n",
    "tweets_df = pd.read_csv(\"dataset/tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In users.csv there are the following variables:\n",
    "1. User Id: a unique identifier of the user\n",
    "2. Statues Count: According to the teacher, this is the count of the tweets made by the user at the moment of data\n",
    "crawling. According to [Twitter API docs](https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/user), this is the number of Tweets (including retweets) issued by the user, but not replies (according to Francesca Naretto); since tweets.csv inclues also users' replies note that **there is no link between the number of tweets for each user in tweets.csv and statuses_count**.\n",
    "3. Lang: the userâ€™s language selected\n",
    "4. Created at: the timestamp in which the profile was created\n",
    "5. Label: a binary variable that indicates if a user is a bot or a genuine user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.info(verbose=True, show_counts=True, memory_usage= \"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute type and quality\n",
    "\n",
    "In the **user** dataset there are 6 columns:\n",
    "\n",
    "1. The id **column** seems to be ok, all values are integer and there are not null values, we have to check possible duplicates\n",
    " \n",
    "2. We have 1 null value in the **name** column, we also assume that the name could be a string, a number or a special character, the names are not necessarily unique, but maybe it's intresting to check the frequency distribution.\n",
    "\n",
    "3. In the **lang** column we don't have null values, but we have to check whether there are problems in the pattern used to express the language, we expect a categorical attribute \n",
    "\n",
    "4. The **bot** column is numerical as expected (binary), we have to check whether all the numbers are 0 or 1\n",
    "\n",
    "5. The attribute **created_at** has no null values, but we have to check the correctness of the date, both sintactic and semantic (not too far in the past or in the future)\n",
    "\n",
    "6. The **status_count** column has 399 of null values, in the non-null values there would semm to be unexpected float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the uniqueness of ids: all the ids are unique!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of total IDs:\", len(users_df[\"id\"]))\n",
    "print(\"Number of unique IDs:\", len(pd.unique(users_df[\"id\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said before one name is null. There are also duplicate names, but this isn't a surprising behaviour, as many people have the same names. By plotting the names' frequencies we can see that there aren't strange phenomena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of total names:\", len(users_df[\"name\"]))\n",
    "print(\"Number of unique names:\", len(pd.unique(users_df[\"name\"])))\n",
    "\n",
    "freq = {}\n",
    "for n in users_df['name']:\n",
    "    if n in freq:\n",
    "        freq[n] += 1\n",
    "    else:\n",
    "        freq[n] = 1\n",
    "\n",
    "number_of_total_names = len(users_df[\"name\"])\n",
    "not_empty_or_missing_names = []\n",
    "empty_or_missing_names = []\n",
    "names_with_only_spaces = []\n",
    "\n",
    "# iterate over all names looking for errors\n",
    "for value in users_df[\"name\"]:\n",
    "    if pd.isna(value) or value == \"\": # name is nan or is_empty string\n",
    "        empty_or_missing_names.append(value)\n",
    "    if str(value).strip() == \"\":\n",
    "            names_with_only_spaces.append(value)\n",
    "    elif not(pd.isna(value) or value == \"\"):\n",
    "        not_empty_or_missing_names.append(value)\n",
    "        \n",
    "print(f\"Number of total names = {number_of_total_names} vs total name values that are not NA or empty = {len(not_empty_or_missing_names)}\")\n",
    "print(f\"Number of total names = {number_of_total_names} vs total name values that are NA or empty = {len(empty_or_missing_names)}\")\n",
    "\n",
    "pd.DataFrame({\"frequencies\": [_ for _ in freq.values()]}).hist(\n",
    "    column=[\"frequencies\"], \n",
    "    log=True, \n",
    "    bins=utils.get_sturges_bins(len(freq.values()))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't see the 1 missing name to be of any significance. So we will just let it be for now. Now let's check the different languages in the \"lang\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(users_df[\"lang\"])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"lang\" field is composed of [IETF language codes](https://en.wikipedia.org/wiki/IETF_language_tag). By selecting only the unique values it's possible to see that there are some erroneous values:\n",
    "* \"Select Language...\" and \"xx-lc\" seems to be **default values**\n",
    "* other values are not properly correct (e.g. \"zh-cn\" instead of \"zh-CN\")\n",
    "We propose to check the most common language used by these 'erroneous values' users and provide them with a more fitting language attribute. This will be done after we have analysed the tweets data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.repair_lang_attribute(users_df)\n",
    "pd.unique(users_df[\"lang\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since wrong values are just the 0.02% of the number of rows they are just dropped *(!!! the actual code in utils.py doesn't drop the default values)*, while the other values are mapped to the correct ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bot attribute is perfectly as expected, all non-null binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(users_df[\"bot\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the created_at coloumn is recognized by pandas as an object, and not as a datetime as we would expect from this attribute. Clean created_at field, by converting string to datetime. We can also note that there are no users whose account was created before (after) the date of creation of the first tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 0 entries\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   id              0 non-null      int64         \n",
      " 1   name            0 non-null      object        \n",
      " 2   lang            0 non-null      object        \n",
      " 3   bot             0 non-null      int64         \n",
      " 4   created_at      0 non-null      datetime64[ns]\n",
      " 5   statuses_count  0 non-null      float64       \n",
      "dtypes: datetime64[ns](1), float64(1), int64(2), object(2)\n",
      "memory usage: 0.0+ bytes\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 0 entries\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   id              0 non-null      int64         \n",
      " 1   name            0 non-null      object        \n",
      " 2   lang            0 non-null      object        \n",
      " 3   bot             0 non-null      int64         \n",
      " 4   created_at      0 non-null      datetime64[ns]\n",
      " 5   statuses_count  0 non-null      float64       \n",
      "dtypes: datetime64[ns](1), float64(1), int64(2), object(2)\n",
      "memory usage: 0.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# parsing string to datetime obj\n",
    "users_df[\"created_at\"] = pd.to_datetime(users_df[\"created_at\"])\n",
    "\n",
    "before_time_users_df = users_df[users_df[\"created_at\"] < datetime(2006,3,21,9,50,0)]\n",
    "before_time_users_df.info()\n",
    "\n",
    "# finding tweets created after dataset release\n",
    "before_time_users_df = users_df[users_df[\"created_at\"] > datetime(2022,9,29,11,0,0)]\n",
    "before_time_users_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the statuses count to be an integer, but pandas has interpreted it as a float. This is probably due to the presence of NaN values. Checking for NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>lang</th>\n",
       "      <th>bot</th>\n",
       "      <th>created_at</th>\n",
       "      <th>statuses_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>616225564</td>\n",
       "      <td>Anisha Williams</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-06-25 15:49:36</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1370348599</td>\n",
       "      <td>Robert Brown</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-23 19:05:48</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>617369459</td>\n",
       "      <td>Sage Pennington</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-06-26 18:50:48</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2362195375</td>\n",
       "      <td>Delaine Nock</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-02-28 05:35:38</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>2351047069</td>\n",
       "      <td>Tosha Pacitti</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-02-21 03:14:40</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11256</th>\n",
       "      <td>2352903248</td>\n",
       "      <td>Margherita Dass</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-02-22 07:59:20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11321</th>\n",
       "      <td>2366095543</td>\n",
       "      <td>Annmarie Willoby</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-03-02 20:53:35</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11332</th>\n",
       "      <td>1176282888</td>\n",
       "      <td>Blanch Mobley</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-02-15 17:38:55</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11391</th>\n",
       "      <td>1175005345</td>\n",
       "      <td>Marna Lemley</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-02-15 11:27:01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11395</th>\n",
       "      <td>2352911527</td>\n",
       "      <td>Alysia Khang</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-02-22 08:01:21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>399 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id              name lang  bot          created_at  \\\n",
       "8       616225564   Anisha Williams   en    0 2017-06-25 15:49:36   \n",
       "20     1370348599      Robert Brown   en    0 2018-04-23 19:05:48   \n",
       "66      617369459   Sage Pennington   en    0 2017-06-26 18:50:48   \n",
       "77     2362195375      Delaine Nock   en    0 2019-02-28 05:35:38   \n",
       "110    2351047069     Tosha Pacitti   en    0 2019-02-21 03:14:40   \n",
       "...           ...               ...  ...  ...                 ...   \n",
       "11256  2352903248   Margherita Dass   en    0 2019-02-22 07:59:20   \n",
       "11321  2366095543  Annmarie Willoby   en    0 2019-03-02 20:53:35   \n",
       "11332  1176282888     Blanch Mobley   en    0 2018-02-15 17:38:55   \n",
       "11391  1175005345      Marna Lemley   en    0 2018-02-15 11:27:01   \n",
       "11395  2352911527      Alysia Khang   en    0 2019-02-22 08:01:21   \n",
       "\n",
       "       statuses_count  \n",
       "8                 NaN  \n",
       "20                NaN  \n",
       "66                NaN  \n",
       "77                NaN  \n",
       "110               NaN  \n",
       "...               ...  \n",
       "11256             NaN  \n",
       "11321             NaN  \n",
       "11332             NaN  \n",
       "11391             NaN  \n",
       "11395             NaN  \n",
       "\n",
       "[399 rows x 6 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df[users_df[\"statuses_count\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.info(verbose=True, show_counts=True, memory_usage= \"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the statuses_count to integer worked. No problems there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of variables and statistics\n",
    "Let's study them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.hist(\n",
    "    column=[\"statuses_count\"], \n",
    "    log=True, \n",
    "    bins=utils.get_sturges_bins(len(users_df[\"statuses_count\"]))\n",
    ")\n",
    "\n",
    "users_df.hist(\n",
    "    column=[\"statuses_count\"], \n",
    "    by=\"bot\", \n",
    "    log=True,\n",
    "    bins=utils.get_sturges_bins(len(users_df[\"statuses_count\"])) #FIX THIS: USES ALL THE SAMPLES, NOT JUST THE BOTS AND THE USERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = pd.unique(users_df[\"lang\"]) \n",
    "bot_freqs = []\n",
    "user_freqs = []\n",
    "for lang in langs:\n",
    "    user_freqs.append(len(users_df.query(f\"lang == '{lang}' & bot == 0\")))\n",
    "    bot_freqs.append(len(users_df.query(f\"lang == '{lang}' & bot == 1\")))\n",
    "langs_df = pd.DataFrame({\"lang\": langs, \"bot_freqs\": bot_freqs, \"user_freqs\": user_freqs})\n",
    "langs_df.plot.bar(x=\"lang\", logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Summary\n",
    "\n",
    "Here we should make a summary of how we believe the data quality for the users are. This could also be done after we have fixed the language issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tweets.csv each row contains information about a single tweet. In this case the variables\n",
    "are:\n",
    "1. ID: a unique identifier for the tweet\n",
    "2. User Id: a unique identifier for the user who wrote the tweet\n",
    "3. Retweet count: number of retweets for the tweet in analysis\n",
    "4. Reply count: number of reply for the tweet in analysis\n",
    "5. Favorite count: number of favorites (likes) received by the tweet\n",
    "6. Num hashtags: number of hashtags used in the tweet\n",
    "7. Num urls: number of urls in the tweet\n",
    "8. Num mentions: number of mentions in the tweet\n",
    "9. Created at: when the tweet was created\n",
    "10. Text: the text of the tweet\n",
    "\n",
    "Regarding the num * fields, we don't have to check the validity of the values and can assume they are correct, except for the null and clear incorrect values. In order to substitute the null values of \"Num hashtags\" field, we can exploit the information of the text (however for the mentions and urls it is impossible to check their validity because we may have a mention to a user that does not exist and we cannot know it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_df.info(verbose=True, show_counts=True, memory_usage= \"deep\") \n",
    "# tweets_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keeping only the tweets with user_id in user dataset. As these are the ones we would like to study, and have the data to verify the validity of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_dropping_rows_number = len(tweets_df.index)\n",
    "\n",
    "numeric_ids = pd.to_numeric(tweets_df[\"user_id\"], errors=\"coerce\")\n",
    "ids_are_not_in_users_df = numeric_ids[numeric_ids.isin(users_df[\"id\"]) == False]\n",
    "tweets_df.drop(ids_are_not_in_users_df.index, inplace=True)\n",
    "\n",
    "tweets_df[\"user_id\"] = pd.to_numeric(tweets_df[\"user_id\"], errors=\"coerce\")\n",
    "\n",
    "after_dropping_rows_number = len(tweets_df.index)\n",
    "\n",
    "print(f\"Percentage of tweets whose author id isn't inside the users dataframe: {(100*(before_dropping_rows_number-after_dropping_rows_number))/(before_dropping_rows_number)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean id field by casting to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[\"id\"] = pd.to_numeric(tweets_df[\"id\"], errors=\"coerce\") # cast field to int and set invalid values to NaN\n",
    "# how to handle duplicates? If there are two different tweets with the same id, how to treat them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing tweets which are duplicates on every attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_number_rows = len(tweets_df.index)\n",
    "\n",
    "all_columns_duplicated_df = tweets_df[tweets_df.duplicated(subset=None, keep=\"first\")]\n",
    "all_columns_duplicated_number = len(all_columns_duplicated_df.index)\n",
    "\n",
    "print(f\"Percentage of tweets duplicated along all the columns that we are deleting(after keeping the first instance): {(100*(all_columns_duplicated_number))/original_number_rows}\")\n",
    "\n",
    "tweets_df.drop(labels=all_columns_duplicated_df.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not finished. It loops forever, so need to look at this some other time. - Tengel\n",
    "nan_id_tweets = tweets_df[\"id\"].isna()\n",
    "#nan_id_tweets.head(1)\n",
    "#print(nan_id_tweets)\n",
    "print(f\"{(100*(len(tweets_df[nan_id_tweets]))/(len(tweets_df)))}\")\n",
    "len(tweets_df[nan_id_tweets])\n",
    "\n",
    "tweets_df.drop(labels=tweets_df[nan_id_tweets].index, inplace=True)\n",
    "len(tweets_df[nan_id_tweets])\n",
    "\"\"\"\n",
    "p = [] # List containing the indexes of all tweets with same IDs, but with differing content where the ID fields are not NaN. Aka ID errors on the twitter dataset.\n",
    "for elem in b:\n",
    "    if tweets_df[tweets_df.index == elem].notna == True:\n",
    "        p.append(elem)\n",
    "\n",
    "\n",
    "p[:10]\n",
    "len(p)\n",
    "print(f\"Tweets with different content on all fields except for ID which are duplicated and not of NaN value. We find {len(p)} number of these tweets. \")\n",
    "print(f\"This corresponds to {(100*(len(p)/len(tweets_df)))} percent of all the tweets\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "studying tweets which are duplicates on all attributes but the tweet id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_equal_on_id = tweets_df[tweets_df.duplicated(subset=\"id\", keep=\"first\")]\n",
    "tweets_equal_on_all_but_id = tweets_df[tweets_df.duplicated(\n",
    "    subset=[\n",
    "        \"user_id\", \"retweet_count\", \"reply_count\", \"favorite_count\", \"num_hashtags\", \"num_urls\", \"num_mentions\", \"created_at\", \"text\"\n",
    "    ], \n",
    "    keep=\"first\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_of_tweets_equal_on_id = [_ for _ in tweets_equal_on_id.index]\n",
    "indexes_of_tweets_equal_on_all_but_id = [_ for _ in tweets_equal_on_all_but_id.index]\n",
    "\n",
    "# List containing the indexes of all tweets equals on all columns, but have not same ID\n",
    "l1 = [\n",
    "    elem \n",
    "    for elem in indexes_of_tweets_equal_on_all_but_id \n",
    "    if elem not in indexes_of_tweets_equal_on_id\n",
    "]\n",
    "        \n",
    "# List containing the indexes of all tweets with same IDs, but with differing columns.\n",
    "l2 = [\n",
    "    elem\n",
    "    for elem in indexes_of_tweets_equal_on_id \n",
    "    if elem not in indexes_of_tweets_equal_on_all_but_id\n",
    "]\n",
    "\n",
    "print(f\"Percentage of tweets with equal id, but different attributes: {(100*(len(l1)))/len(tweets_df)}\")\n",
    "print(f\"Percentage of tweets with equal attributes, but different id: {(100*(len(l2)))/len(tweets_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.drop(labels=tweets_equal_on_id.index, inplace=True)\n",
    "tweets_equal_on_id = tweets_df[tweets_df.duplicated(subset=\"id\", keep=\"first\")]\n",
    "print(f\"Percentage of tweets with equal id, but different attributes: {(100*(len(tweets_equal_on_id)))/len(tweets_df)}\")\n",
    "tweets_equal_on_id.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to drop all tweets with NaN values as their ID. As it is difficult for us to ensure their validity. We have also decided to drop all tweets with duplicated IDs. Keeping only its first instance. \n",
    "\n",
    "As we can see from the data above. Pandas treats tweets with fields containing NaN values as different from each other. Atleast when it comes to ID. \n",
    "\n",
    "Lets see if we can find the tweets with real ID values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"retweet_count\", \"reply_count\", \"favorite_count\", \"num_hashtags\", \"num_urls\", \"num_mentions\"]\n",
    "tweets_df[columns].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first observation that can be made is that \\*_count and num_\\* fields should contain only positive integers. \n",
    "The second one, by looking on the table above, is that there are some very strange values like inf or 7e+211\n",
    "\n",
    "The following thresholds are based on the most retweet and \"liked\" tweets on the platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\n",
    "    \"retweet_count\": 3738380,\n",
    "    \"favorite_count\": 7114892\n",
    "}\n",
    "\n",
    "print(\"Number of invalid values for the following columns:\")\n",
    "for column_name in columns:\n",
    "    # casting all the columns to numeric (and setting invalid value to null)\n",
    "    tweets_df[column_name] = pd.to_numeric(tweets_df[column_name], errors=\"coerce\")\n",
    "\n",
    "    print(f\"{column_name}\")\n",
    "    # evaluating the presence of negative values\n",
    "    negative_series = tweets_df[tweets_df[column_name] < 0][column_name]\n",
    "    print(f\"\\tnegative: {negative_series.size}\")\n",
    "    tweets_df[column_name].loc[negative_series.index] = np.NaN # setting negative values to NaN\n",
    "    \n",
    "    # evaluating the presence of inf values\n",
    "    inf_series = utils.get_inf_elements(tweets_df[column_name])\n",
    "    print(f\"\\tinf: {inf_series.size}\")\n",
    "    tweets_df[column_name].loc[inf_series.index] = np.NaN # setting inf values to NaN\n",
    "\n",
    "    # evaluating the presence of decimal values\n",
    "    is_float = lambda n: (not pd.isna(n)) and (not math.isinf(n)) and (not n.is_integer())\n",
    "    decimal_values_series =  tweets_df[column_name].apply(is_float)\n",
    "    print(f\"\\tdecimals: {tweets_df[column_name][decimal_values_series].size}\")\n",
    "    \n",
    "    # finding values above given thresholds (if specified)\n",
    "    if column_name in thresholds:\n",
    "        threshold = thresholds[column_name]\n",
    "        above_threshold_series = tweets_df[tweets_df[column_name] > threshold ][column_name]\n",
    "        print(f\"\\tvalues above threshold ({threshold}): {above_threshold_series[above_threshold_series > threshold].size}\") \n",
    "        tweets_df[column_name].loc[above_threshold_series.index] = np.NaN # setting values above threshold to nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "created_at should be datetime\n",
    "\n",
    "Checks if all the tweets were created after the first tweet published on twitter (so we don't have something strange like a tweet created in 01-01-1990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_df[\"created_at\"] = pd.to_datetime(tweets_df[\"created_at\"], errors=\"coerce\")\n",
    "\n",
    "# finding tweets created before twitter first tweet\n",
    "before_time_tweets_df = tweets_df[tweets_df[\"created_at\"] < datetime(2006,3,21,12,50,0)]\n",
    "before_time_tweets_df.info()\n",
    "\n",
    "# finding tweets created after dataset release\n",
    "before_time_tweets_df = tweets_df[tweets_df[\"created_at\"] > datetime(2022,9,29,11,0,0)]\n",
    "before_time_tweets_df.info()\n",
    "\n",
    "# what to do with these tweets? I suggest dropping/removing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above values, there is 48803 tweets that were created in what we would call \"pre-history\" and 48802 tweets that where made in \"the future\". A time after the release date of this dataset. These tweets will have to be removed from the final dataset of \"valid\" tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.info(verbose=True, show_counts=True, memory_usage= \"deep\") \n",
    "tweets_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing tweets with null text field, float text field or only spaces, because these are not allowed by twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tweets_df.dropna(subset=[\"text\"], inplace=True) # drop the tweets where the text field is null\n",
    "tweets_df[\"text\"] = tweets_df[\"text\"].astype(str) # cast the test field to string\n",
    "\n",
    "\n",
    "# Problem with memory usage, we have to find a more effient way to remove tweets with only spaces\n",
    "\"\"\"\n",
    "to_drop = []\n",
    "for index, row in tweets_df.iterrows():\n",
    "    if not row[\"text\"] or row[\"text\"].isspace():\n",
    "        tweets_df.drop(index)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of variables and statistics\n",
    "Let's study them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms for numerical fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# REPLACING INF TO NAN; THIS IS JUST A TEMPORARY FUNCTION I PUT HERE IN ORDER TO PLOT THE HISTOGRAMS - Gianluca\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100 # 200 e.g. is really fine, but slower\n",
    "####\n",
    "tweets_df.hist(\n",
    "    column=columns, \n",
    "    log=True,\n",
    "    bins=utils.get_sturges_bins(tweets_df.size)\n",
    ")\n",
    "\"\"\"\n",
    "columns = [\"retweet_count\", \"reply_count\", \"favorite_count\", \"num_hashtags\", \"num_urls\", \"num_mentions\"]\n",
    "\n",
    "tweets_df.hist(\n",
    "    column=columns, \n",
    "    log=True,\n",
    "    #bins=utils.get_sturges_bins(tweets_df.size)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplots for numerical fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns:\n",
    "    tweets_df.plot(kind=\"box\",\n",
    "        column=column,\n",
    "        logy=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables trasformations (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_df = pd.merge(users_df, tweets_df, left_on=[\"id\"], right_on=[\"user_id\"],)\n",
    "\n",
    "merged_df.drop(columns=\"user_id\", inplace=True)\n",
    "merged_df.rename(columns={\"id_x\" : \"user_id\"}, inplace=True)\n",
    "merged_df.rename(columns={\"id_y\" : \"tweet_id\"}, inplace=True)\n",
    "merged_df.rename(columns={\"created_at_x\" : \"account_created\"}, inplace=True)\n",
    "merged_df.rename(columns={\"created_at_y\" : \"tweet_created\"}, inplace=True)\n",
    "merged_df.head(1)\n",
    "\n",
    "merged_df.to_csv(\"./dataset/merged_dataset.csv\",index=False) # Removes the counting of the Index rows\n",
    "#merged_df = tweets_df.join(users_df, on=\"id\")\n",
    "#merged_df.head(2)\n",
    "#\n",
    "\n",
    "\n",
    "#works well but problems with wrong values (like float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "beedbe2faf2f7048d727558d0bc3221e7eba2a0b921cac4d4771b2feb8f74b30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
