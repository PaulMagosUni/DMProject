{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import classification_utils\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./dataset/users_df_dataset_cleaned_with_indicators.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use only the numerical attributes, except for the bot column that is the target one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\n",
    "#    \"user_id\"  ,\n",
    "#    \"name\"  ,\n",
    "#    \"lang\"  ,\n",
    "    \"bot\"  ,\n",
    "#    \"created_at\" ,\n",
    "    \"statuses_count\" ,\n",
    "    \"account_age_in_days\" ,\n",
    "    \"number_of_tweets\" ,\n",
    "    \"account_average_tweets_per_day\" ,\n",
    "    \"avg_tweets_per_actual_day\" ,\n",
    "#    \"day_with_most_tweets\" ,\n",
    "    \"max_number_of_tweets_in_a_day\",\n",
    "    \"entropy_for_day\",\n",
    "    \"entropy_for_hour\",\n",
    "    \"entropy_for_minute\",\n",
    "    \"avg_hashtags\",\n",
    "    \"avg_text_length\",\n",
    "    \"avg_mentions\",\n",
    "    \"avg_special_char_in_text\",\n",
    "    \"total_likes\",\n",
    "    \"avg_favorite_count\",\n",
    "    \"total_replies\",\n",
    "    \"avg_reply_count\",\n",
    "    \"total_retweet_count\",\n",
    "    \"account_discussion_creation_ratio\",\n",
    "    \"tweet_num_likes_ratio\",\n",
    "    \"tweet_num_replies_ratio\",\n",
    "    \"entropy_original_text\",\n",
    "    \"entropy_text\",\n",
    "    \"mean_inactive_period_length_in_seconds\",\n",
    "    \"median_inactive_period_length_in_seconds\",\n",
    "    \"mode_inactive_period_length_in_seconds\",\n",
    "    \"mode_count\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next 3 lines we shuffle the data in order to avoid biases if for example all the bot are in the bottom part and they will be part only of the test set, then we split the bot column from the other ones and convert them to a tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.sample(frac=1, random_state=1)\n",
    "#target_array = tf.convert_to_tensor(df.pop(\"bot\"))\n",
    "#feature_matrix = tf.convert_to_tensor(df)\n",
    "\n",
    "df = df.sample(frac=1, random_state=1)\n",
    "target_array = df.pop(\"bot\").values\n",
    "feature_matrix = df.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>account_age_in_days</th>\n",
       "      <th>number_of_tweets</th>\n",
       "      <th>account_average_tweets_per_day</th>\n",
       "      <th>avg_tweets_per_actual_day</th>\n",
       "      <th>max_number_of_tweets_in_a_day</th>\n",
       "      <th>entropy_for_day</th>\n",
       "      <th>entropy_for_hour</th>\n",
       "      <th>entropy_for_minute</th>\n",
       "      <th>avg_hashtags</th>\n",
       "      <th>...</th>\n",
       "      <th>total_retweet_count</th>\n",
       "      <th>account_discussion_creation_ratio</th>\n",
       "      <th>tweet_num_likes_ratio</th>\n",
       "      <th>tweet_num_replies_ratio</th>\n",
       "      <th>entropy_original_text</th>\n",
       "      <th>entropy_text</th>\n",
       "      <th>mean_inactive_period_length_in_seconds</th>\n",
       "      <th>median_inactive_period_length_in_seconds</th>\n",
       "      <th>mode_inactive_period_length_in_seconds</th>\n",
       "      <th>mode_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2987</th>\n",
       "      <td>68</td>\n",
       "      <td>2163</td>\n",
       "      <td>2074</td>\n",
       "      <td>0.031438</td>\n",
       "      <td>10.690722</td>\n",
       "      <td>43</td>\n",
       "      <td>4.577648</td>\n",
       "      <td>2.202349</td>\n",
       "      <td>0.822681</td>\n",
       "      <td>0.248795</td>\n",
       "      <td>...</td>\n",
       "      <td>1334112</td>\n",
       "      <td>0.001555</td>\n",
       "      <td>3.405583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.853126</td>\n",
       "      <td>2.747561e-07</td>\n",
       "      <td>53500.641273</td>\n",
       "      <td>845.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>68</td>\n",
       "      <td>1005</td>\n",
       "      <td>3457</td>\n",
       "      <td>0.067662</td>\n",
       "      <td>86.425000</td>\n",
       "      <td>224</td>\n",
       "      <td>5.103056</td>\n",
       "      <td>4.158180</td>\n",
       "      <td>1.425546</td>\n",
       "      <td>0.014753</td>\n",
       "      <td>...</td>\n",
       "      <td>377448</td>\n",
       "      <td>0.009159</td>\n",
       "      <td>1.328593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.556204</td>\n",
       "      <td>2.491898e-07</td>\n",
       "      <td>3151.758461</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8447</th>\n",
       "      <td>1785</td>\n",
       "      <td>2521</td>\n",
       "      <td>1238</td>\n",
       "      <td>0.708052</td>\n",
       "      <td>3.134177</td>\n",
       "      <td>21</td>\n",
       "      <td>2.742274</td>\n",
       "      <td>1.435677</td>\n",
       "      <td>0.466968</td>\n",
       "      <td>0.371567</td>\n",
       "      <td>...</td>\n",
       "      <td>133842</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>77.375000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.126784</td>\n",
       "      <td>2.269985e-07</td>\n",
       "      <td>114544.543619</td>\n",
       "      <td>5181.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7109</th>\n",
       "      <td>68</td>\n",
       "      <td>3078</td>\n",
       "      <td>2254</td>\n",
       "      <td>0.022092</td>\n",
       "      <td>7.772414</td>\n",
       "      <td>59</td>\n",
       "      <td>4.183229</td>\n",
       "      <td>2.123818</td>\n",
       "      <td>0.763846</td>\n",
       "      <td>0.526619</td>\n",
       "      <td>...</td>\n",
       "      <td>167190</td>\n",
       "      <td>0.013482</td>\n",
       "      <td>2.782716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.998077</td>\n",
       "      <td>2.118565e-07</td>\n",
       "      <td>84270.544809</td>\n",
       "      <td>919.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2519</th>\n",
       "      <td>68</td>\n",
       "      <td>2017</td>\n",
       "      <td>3010</td>\n",
       "      <td>0.033713</td>\n",
       "      <td>25.083333</td>\n",
       "      <td>86</td>\n",
       "      <td>5.359900</td>\n",
       "      <td>3.144455</td>\n",
       "      <td>0.907828</td>\n",
       "      <td>0.018272</td>\n",
       "      <td>...</td>\n",
       "      <td>553115</td>\n",
       "      <td>0.005442</td>\n",
       "      <td>1.718037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.390271</td>\n",
       "      <td>3.563860e-07</td>\n",
       "      <td>32652.958472</td>\n",
       "      <td>197.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      statuses_count  account_age_in_days  number_of_tweets  \\\n",
       "2987              68                 2163              2074   \n",
       "7999              68                 1005              3457   \n",
       "8447            1785                 2521              1238   \n",
       "7109              68                 3078              2254   \n",
       "2519              68                 2017              3010   \n",
       "\n",
       "      account_average_tweets_per_day  avg_tweets_per_actual_day  \\\n",
       "2987                        0.031438                  10.690722   \n",
       "7999                        0.067662                  86.425000   \n",
       "8447                        0.708052                   3.134177   \n",
       "7109                        0.022092                   7.772414   \n",
       "2519                        0.033713                  25.083333   \n",
       "\n",
       "      max_number_of_tweets_in_a_day  entropy_for_day  entropy_for_hour  \\\n",
       "2987                             43         4.577648          2.202349   \n",
       "7999                            224         5.103056          4.158180   \n",
       "8447                             21         2.742274          1.435677   \n",
       "7109                             59         4.183229          2.123818   \n",
       "2519                             86         5.359900          3.144455   \n",
       "\n",
       "      entropy_for_minute  avg_hashtags  ...  total_retweet_count  \\\n",
       "2987            0.822681      0.248795  ...              1334112   \n",
       "7999            1.425546      0.014753  ...               377448   \n",
       "8447            0.466968      0.371567  ...               133842   \n",
       "7109            0.763846      0.526619  ...               167190   \n",
       "2519            0.907828      0.018272  ...               553115   \n",
       "\n",
       "      account_discussion_creation_ratio  tweet_num_likes_ratio  \\\n",
       "2987                           0.001555               3.405583   \n",
       "7999                           0.009159               1.328593   \n",
       "8447                           0.009250              77.375000   \n",
       "7109                           0.013482               2.782716   \n",
       "2519                           0.005442               1.718037   \n",
       "\n",
       "      tweet_num_replies_ratio  entropy_original_text  entropy_text  \\\n",
       "2987                      0.0              10.853126  2.747561e-07   \n",
       "7999                      0.0              11.556204  2.491898e-07   \n",
       "8447                      0.0              10.126784  2.269985e-07   \n",
       "7109                      0.0              10.998077  2.118565e-07   \n",
       "2519                      0.0              11.390271  3.563860e-07   \n",
       "\n",
       "      mean_inactive_period_length_in_seconds  \\\n",
       "2987                            53500.641273   \n",
       "7999                             3151.758461   \n",
       "8447                           114544.543619   \n",
       "7109                            84270.544809   \n",
       "2519                            32652.958472   \n",
       "\n",
       "      median_inactive_period_length_in_seconds  \\\n",
       "2987                                     845.5   \n",
       "7999                                      56.0   \n",
       "8447                                    5181.0   \n",
       "7109                                     919.5   \n",
       "2519                                     197.0   \n",
       "\n",
       "      mode_inactive_period_length_in_seconds  mode_count  \n",
       "2987                                     0.0         153  \n",
       "7999                                     0.0         282  \n",
       "8447                                     0.0          91  \n",
       "7109                                     0.0         171  \n",
       "2519                                     0.0         227  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data in training, validation and test.\n",
    "The training data are the only ones used to fit the neural network, the expactation is that the classification error on this set will only decrease durning the training.\n",
    "\n",
    "The validation data are used to itaratively evaluate the network and select the best architecture, if the classification error on the validation set increases, it means that the network is in overfit.\n",
    "\n",
    "The test set is used only after the choise of the final model and is used to have an idea of the error of the network on completely new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_size = 0.7\n",
    "vl_size = 0.2\n",
    "ts_size = 0.1\n",
    "\n",
    "tr_index = round(len(feature_matrix) * tr_size)\n",
    "vl_index = tr_index + round(len(feature_matrix) * vl_size)\n",
    "ts_index = vl_index + round(len(feature_matrix) * ts_size)\n",
    "\n",
    "feature_matrix_tr = feature_matrix[0:tr_index]\n",
    "target_array_tr = target_array[0:tr_index]\n",
    "\n",
    "feature_matrix_vl = feature_matrix[tr_index:vl_index]\n",
    "target_array_vl = target_array[tr_index:vl_index]\n",
    "\n",
    "feature_matrix_ts = feature_matrix[vl_index:ts_index]\n",
    "target_array_ts = target_array[vl_index:ts_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7776, 27)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choose of the neural network architecture is guided by the theory: since we already have processed features (and not row data), and there is no sign of a hierarchy of features to exploit with the inductive bias of a deep neural network (as for the image classification), we choose to adopt a shallow model, with only two hiddel layer.\n",
    "\n",
    "https://www.deeplearningbook.org/\n",
    "\n",
    "https://www.nature.com/articles/nature14539\n",
    "\n",
    "In the input layer we normalize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO improve with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "Epoch 1/400\n",
      "Epoch 1/400\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the learning rate and momentum\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def create_model(epochs=100, batch_size=4096, activation=\"relu\", num_neurons=32, learning_rate=0.1, momentum=0.1):\n",
    "    in_layer = tf.keras.layers.Normalization(axis=-1)\n",
    "    in_layer.adapt(feature_matrix_tr)\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(in_layer)\n",
    "    model.add(tf.keras.layers.Dense(num_neurons, activation=activation))  \n",
    "    model.add(tf.keras.layers.Dense(num_neurons, activation=activation))\n",
    "    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "        optimizer=tf.keras.optimizers.SGD(\n",
    "            learning_rate=learning_rate,\n",
    "            momentum=momentum\n",
    "        )\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(model=create_model)\n",
    "\n",
    "# define the grid search parameters\n",
    "learning_rate = [0.01, 0.1, 0.2]\n",
    "momentum = [0.2 , 0.4, 0.6]\n",
    "epochs = [400, 500, 600]\n",
    "batch_size = [2048, 4096]\n",
    "activation =[\"sigmoid\", \"relu\"]\n",
    "num_neurons = [32, 64]\n",
    "\n",
    "param_grid = dict(\n",
    "    #with the keyword \"model__\" the parameter is used in the \"create_model\" function\n",
    "    model__learning_rate=learning_rate, \n",
    "    model__momentum=momentum,\n",
    "    model__activation = activation,\n",
    "    model__num_neurons=num_neurons,\n",
    "    #without the keyword the parameter is used in the \"fit\" function\n",
    "    epochs = epochs,\n",
    "    batch_size = batch_size\n",
    "    )\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=10, cv=3)\n",
    "grid_result = grid.fit(feature_matrix_tr , target_array_tr)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 4096, 'epochs': 400, 'model__activation': 'sigmoid', 'model__learning_rate': 0.1, 'model__momentum': 0.4, 'model__num_neurons': 64}\n",
      "0.8299897119341564\n"
     ]
    }
   ],
   "source": [
    "model = grid.best_estimator_\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_layer = tf.keras.layers.Normalization(axis=-1)\n",
    "in_layer.adapt(feature_matrix_tr)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  in_layer,\n",
    "  tf.keras.layers.Dense(64, activation='sigmoid'),   \n",
    "  tf.keras.layers.Dense(64, activation='sigmoid'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "  feature_matrix_tr,\n",
    "  target_array_tr,\n",
    "  batch_size=2048,\n",
    "  epochs=600,\n",
    "  validation_data=(feature_matrix_vl, target_array_vl),\n",
    "  shuffle=True,\n",
    "  verbose=False,\n",
    "  class_weight={\n",
    "    0 : 1.1,\n",
    "    1 : 1\n",
    "  }\n",
    ")\n",
    "\n",
    "classification_utils.print_training_stats(history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('Accuracy train set ', metrics.accuracy_score(target_array_tr, model.predict(feature_matrix_tr) >= 0.5 ))\n",
    "print('Accuracy validation set ', metrics.accuracy_score(target_array_vl, model.predict(feature_matrix_vl) >= 0.5 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we are able to achive a really good accuracy (near 85%), without overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a counter example,in the following picture we want to demostrate that a larger nework is able to achive a near 100% accuracy, but then the generalization capabilities are really low because of overfitting, infact the classification error on the validation begin to increase.\n",
    "\n",
    "We moved the code on anothe file for clarity. The network has 5 hiddel layers with 512 neurons for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_utils.large_model_example(\n",
    "    feature_matrix_tr,\n",
    "    target_array_tr,\n",
    "    feature_matrix_vl,\n",
    "    target_array_vl\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we evaluate the model on completely unseen data, this is like to predict if a new user is a bot or not, i.e. the final objective of the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = (model.predict(feature_matrix_ts) > 0.5).astype(\"bool\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(target_array_ts, test_predict)\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems very good at indentify true bots, while has a little bias to classify real users as boot too (false positive). This can be a consequence of the unbalanceness of the dataset (5000 humans, 6000 bots), but this is already partially correct by the class weight during the fit of the model. A greater weight toward the \"human\" class proved to be worse during the validation phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('Accuracy test set ', metrics.accuracy_score(target_array_ts, test_predict))\n",
    "print('Precision test set ', metrics.precision_score(target_array_ts, test_predict))\n",
    "print('Recall test set ', metrics.recall_score(target_array_ts, test_predict))\n",
    "print('F1 score test set ', metrics.f1_score(target_array_ts, test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_filepath = \"models/nn_model\"\n",
    "model.save(saved_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
