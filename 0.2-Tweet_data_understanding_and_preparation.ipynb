{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all' # to print multiple outputs from the same cell\n",
    "import math\n",
    "import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from operator import index\n",
    "from collections import defaultdict\n",
    "from scipy.stats import pearsonr\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(\"dataset/tweets.csv\")\n",
    "users_df = pd.read_csv(\"dataset/users_dataset_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets Data Understanding and Preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tweets.csv each row contains information about a single tweet. There are 10 columns and In this case the variables are:\n",
    "\n",
    "1. ID: a unique identifier for the tweet\n",
    "\n",
    "2. User Id: a unique identifier for the user who wrote the tweet\n",
    "\n",
    "3. Retweet count: number of retweets for the tweet in analysis\n",
    "\n",
    "4. Reply count: number of reply for the tweet in analysis\n",
    "\n",
    "5. Favorite count: number of favorites (likes) received by the tweet\n",
    "\n",
    "6. Num hashtags: number of hashtags used in the tweet\n",
    "\n",
    "7. Num urls: number of urls in the tweet\n",
    "\n",
    "8. Num mentions: number of mentions in the tweet\n",
    "\n",
    "9. Created at: when the tweet was created\n",
    "\n",
    "10. Text: the text of the tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute type and quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.info(verbose=True, show_counts=True, memory_usage= \"deep\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tweet_id & User_id Columns\n",
    "\n",
    "keeping only the tweets with user_id in user dataset. As these are the ones we would like to study, and have the data to verify the validity of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.rename(columns= {\"id\" : \"tweet_id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_dropping_rows_number = len(tweets_df.index)\n",
    "\n",
    "numeric_ids = pd.to_numeric(tweets_df[\"user_id\"], errors=\"coerce\")\n",
    "ids_are_not_in_users_df = numeric_ids[numeric_ids.isin(users_df[\"user_id\"]) == False]\n",
    "tweets_df.drop(ids_are_not_in_users_df.index, inplace=True)\n",
    "\n",
    "tweets_df[\"user_id\"] = pd.to_numeric(tweets_df[\"user_id\"], errors=\"coerce\")\n",
    "\n",
    "after_dropping_rows_number = len(tweets_df.index)\n",
    "\n",
    "print(f\"Percentage of tweets whose author id isn't inside the users dataframe: {(100*(before_dropping_rows_number-after_dropping_rows_number))/(before_dropping_rows_number)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tweets are dropped from the tweets dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean id field by casting to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[\"tweet_id\"] = pd.to_numeric(tweets_df[\"tweet_id\"], errors=\"coerce\") # cast field to int and set invalid values to NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing tweets which are duplicates on every attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_number_rows = len(tweets_df.index)\n",
    "\n",
    "all_columns_duplicated_df = tweets_df[tweets_df.duplicated(subset=None, keep=\"first\")]\n",
    "all_columns_duplicated_number = len(all_columns_duplicated_df.index)\n",
    "\n",
    "print(f\"Percentage of tweets duplicated along all the columns that we are deleting(after keeping the first instance): {(100*(all_columns_duplicated_number))/original_number_rows}\")\n",
    "\n",
    "tweets_df.drop(labels=all_columns_duplicated_df.index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping tweets whose id is an invalid value (i.e. NaN, duplicate, inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = tweets_df.size\n",
    "tweets_df = tweets_df[tweets_df['tweet_id'].notna()]\n",
    "after= tweets_df.size\n",
    "print(f\"Percentage of tweets with nan tweet_id dropped: {(100*(before - after))/before}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = tweets_df.size\n",
    "tweets_df = tweets_df.drop_duplicates(subset=\"tweet_id\")\n",
    "after = tweets_df.size\n",
    "print(f\"Num of tweets with same tweet_id dropped: {(before - after)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = tweets_df.size\n",
    "tweets_df = tweets_df[tweets_df['tweet_id']!=np.inf]\n",
    "after = tweets_df.size\n",
    "print(f\"Num of tweets with inf tweet_id dropped: {(before - after)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to drop all tweets with NaN values as their ID. As it is difficult for us to ensure their validity. We have also decided to drop all tweets with duplicated IDs. Keeping only its first instance. \n",
    "\n",
    "As we can see from the data above. Pandas treats tweets with fields containing NaN values as different from each other. Atleast when it comes to ID. \n",
    "\n",
    "Lets see if we can find the tweets with non-NaN user-id values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = tweets_df.size\n",
    "tweets_df = tweets_df[tweets_df['user_id'].notna()]\n",
    "after= tweets_df.size\n",
    "print(f\"Percentage of tweets with nan user_id dropped: {(100*(before - after))/before}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert tweet_id attribute to int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[\"tweet_id\"] = tweets_df[\"tweet_id\"].astype(np.int64)\n",
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Numerical columns\n",
    "\n",
    "- Retweet_count\n",
    "- Reply_count\n",
    "- favorite_count\n",
    "- Num_hashtags\n",
    "- Num_urls\n",
    "- num_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"retweet_count\", \"reply_count\", \"favorite_count\", \"num_hashtags\", \"num_urls\", \"num_mentions\"]\n",
    "tweets_df[columns].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following thresholds are based on the most retweet and \"liked\" tweets on the twitter platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\n",
    "   \"retweet_count\": 3738380,\n",
    "   \"favorite_count\": 7114892,\n",
    "   \"reply_count\" : 4000000,\n",
    "   \"num_hashtags\" : 93,\n",
    "   \"num_urls\" : 23,\n",
    "   \"num_mentions\": 93\n",
    "}\n",
    "\n",
    "print(\"Number of invalid values for the following columns:\")\n",
    "for column_name in columns:\n",
    "   # casting all the columns to numeric (and setting invalid value to null)\n",
    "   tweets_df[column_name] = pd.to_numeric(tweets_df[column_name], errors=\"coerce\")\n",
    "\n",
    "   print(f\"{column_name}\")\n",
    "   # evaluating the presence of negative values\n",
    "   negative_series = tweets_df[tweets_df[column_name] < 0][column_name]\n",
    "   print(f\"\\tnegative: {len(negative_series)}\")\n",
    "   tweets_df[column_name].loc[negative_series.index] = np.NaN # setting negative values to NaN\n",
    "   \n",
    "   # evaluating the presence of inf values\n",
    "   inf_series = utils.get_inf_elements(tweets_df[column_name])\n",
    "   print(f\"\\tinf: {len(inf_series)}\")\n",
    "   tweets_df[column_name].loc[inf_series.index] = np.NaN # setting inf values to NaN\n",
    "\n",
    "   # evaluating the presence of decimal values\n",
    "   is_float = lambda n: (not pd.isna(n)) and (not math.isinf(n)) and (not n.is_integer())\n",
    "   decimal_values_series =  tweets_df[column_name].apply(is_float)\n",
    "   print(f\"\\tdecimals: {len(tweets_df[column_name][decimal_values_series])}\")\n",
    "   \n",
    "   # finding values above given thresholds (if specified)\n",
    "   if column_name in thresholds:\n",
    "       threshold = thresholds[column_name]\n",
    "       above_threshold_series = tweets_df[tweets_df[column_name] > threshold ][column_name]\n",
    "       print(f\"\\tvalues above threshold ({threshold}): {len(above_threshold_series[above_threshold_series > threshold])}\") \n",
    "       tweets_df[column_name].loc[above_threshold_series.index] = np.NaN # setting values above threshold to nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first observation that can be made is that \\*_count and num_\\* fields should contain only positive integers. Something that they do.\n",
    "We find some inf values in the following columns:\n",
    "- retweet_count\n",
    "- reply_count\n",
    "\n",
    "We also find values that are above our treshold range: \n",
    "- 36 tweets above the retweet_count treshold\n",
    "- 34 tweets above the favorite_count treshold\n",
    "\n",
    "\n",
    "All these values and the infinate values are set to NaN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing columns to integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting NaN values with median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[columns].mean()\n",
    "\n",
    "for column in columns:\n",
    "    # column_median_value = tweets_df[column].median()\n",
    "    # Set to nan the values higher than the max integer \n",
    "    # Probably noise values\n",
    "    tweets_df[column].mean()\n",
    "    tweets_df[tweets_df[column] > float(np.iinfo(np.int64).max)][column] = np.nan\n",
    "    # Nans to median\n",
    "    # tweets_df.replace( to_replace = np.nan, value = column_median_value, inplace = True )\n",
    "    tweets_df[column] = tweets_df.groupby(\"user_id\")[column].apply(lambda x: x.fillna(int(x.median())))\n",
    "    # Cast column to int\n",
    "    tweets_df[column] = tweets_df[column].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[columns].quantile([0.25, 0.5, 0.75, 0.87, 0.95, 0.99, 0.999, 0.9999, 0.99999, 0.999999])\n",
    "tweets_df[columns].mean()\n",
    "tweets_df[columns].median()\n",
    "tweets_df[columns].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Created_at Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "created_at should be datetime\n",
    "\n",
    "Checks if all the tweets were created after the first tweet published on twitter (so we don't have something strange like a tweet created in 01-01-1990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[\"created_at\"] = pd.to_datetime(tweets_df[\"created_at\"], errors=\"coerce\")\n",
    "\n",
    "# finding tweets created before twitter first tweet\n",
    "before_time_tweets_df = tweets_df[tweets_df[\"created_at\"] < datetime(2006,3,21,12,50,0)]\n",
    "\n",
    "# finding tweets created after dataset release\n",
    "after_time_tweets_df = tweets_df[tweets_df[\"created_at\"] > datetime(2022,9,29,11,0,0)]\n",
    "\n",
    "# dropping out of range tweets\n",
    "tweets_df = tweets_df.drop(before_time_tweets_df.index)\n",
    "tweets_df = tweets_df.drop(after_time_tweets_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.info(verbose=True, show_counts=True, memory_usage= \"deep\") \n",
    "tweets_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing tweets with null text field, float text field or only spaces, because these are not allowed by twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.dropna(subset=[\"text\"], inplace=True) # drop the tweets where the text field is null\n",
    "tweets_df[\"text\"] = tweets_df[\"text\"].astype(str) # cast the text field to string\n",
    "tweets_df = tweets_df[~tweets_df.text.str.isspace()]\n",
    "\n",
    "tweets_df.info(verbose=True, show_counts=True, memory_usage= \"deep\") \n",
    "tweets_df.describe()\n",
    "# Problem with memory usage, we have to find a more effient way to remove tweets with only spaces\n",
    "# We should also drop the tweets only containing the U+3164 HANGUL FILLER, or other invisible charackters.\n",
    "#Source: https://invisible-characters.com/\n",
    "\"\"\"\n",
    "to_drop = []\n",
    "for index, row in tweets_df.iterrows():\n",
    "    if row[\"text\"].isspace():\n",
    "        tweets_df.drop(index)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of variables and statistics\n",
    "Let's study them!\n",
    "\n",
    "Histograms for numerical fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"retweet_count\", \"reply_count\", \"favorite_count\", \"num_hashtags\", \"num_urls\", \"num_mentions\"]\n",
    "\n",
    "tweets_df.hist(\n",
    "    column=columns, figsize=(10,8),\n",
    "    log=True,\n",
    "    #bins=utils.get_sturges_bins(tweets_df.size)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplots for the numerical fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[columns].plot(kind=\"box\",\n",
    "        figsize=(10,8),\n",
    "        logy=True, \n",
    "    )\n",
    "\"\"\"\n",
    "for column in columns:\n",
    "    tweets_df.plot(kind=\"box\",\n",
    "        column=column,\n",
    "        logy=True\n",
    "    )\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #tweets_df[columns].quantile([0.05,0.25,0.5,0.75,0.95, 0.97,0.99])\n",
    "# #tweets_df[tweets_df[columns] != np.nan][columns].mean()\n",
    "\n",
    "# tweets_df[tweets_df[\"retweet_count\"] == \"NaN\"]\n",
    "# tweets_df[tweets_df[\"reply_count\"] == np.nan]\n",
    "# tweets_df[tweets_df[\"favorite_count\"] == np.nan]\n",
    "# tweets_df[tweets_df[\"num_hashtags\"] == np.nan]#[\"retweet_count\"]\n",
    "# tweets_df[tweets_df[\"num_urls\"] == np.nan]\n",
    "# tweets_df[tweets_df[\"num_mentions\"] == np.nan]\n",
    "# #tweets_df[columns].loc[:, tweets_df[columns].isna().any()]\n",
    "\n",
    "# tweets_df.isna()\n",
    "\n",
    "tweets_df[columns].mean()\n",
    "tweets_df[columns].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[tweets_df['reply_count'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the boxplots we can see that there is a very high variance for the data (an observation supported also by the previous histograms). Hence, we compute the whiskers upper and lower bounds analitically for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier detection\n",
    "outlier_thresholds = {\n",
    "    column: utils.compute_whiskers(tweets_df[column])\n",
    "    for column in columns\n",
    "}\n",
    "\n",
    "outlier_dataframes = {\n",
    "    column: tweets_df[tweets_df[column] > outlier_thresholds[column][1]][column]\n",
    "    for column in columns\n",
    "}\n",
    "\n",
    "for column in columns:\n",
    "    print(f\"{column}: {len(outlier_dataframes[column])} outliers\")\n",
    "\n",
    "# Should add what percentage of the dataframe that this consists of.\n",
    "# In this way we get a feeling of how many outliers our cleaned dataset consists of\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These outliers may be removed from the dataframe by running the code block bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # removal of outliers\n",
    "# outlier_indexes_list = [outlier_dataframe.index for outlier_dataframe in outlier_dataframes.values()]\n",
    "# for indexes in outlier_indexes_list:\n",
    "#     tweets_df.drop(indexes, inplace=True, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Data Quality Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.info(verbose=True, show_counts=True, memory_usage= \"deep\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This codeblock should compute the number of tweets belonging to human and bot accounts\n",
    "# It should also compute the percentage of this data, to come up with a number for balance\n",
    "\n",
    "# We also have to decide if we should remove the remaining tweets that include NaN values.\n",
    "# These are around 400 thousand of our 10 million four hundred thousand remaining tweets.\n",
    "# So basically less than 4%\n",
    "\n",
    "# Number_of_users_in_dataset_that_are_bots = users_df.groupby(\"bot\").get_group(1)\n",
    "# Number_of_users_in_dataset_that_are_humans = users_df.groupby(\"bot\").get_group(0)\n",
    "# total = len(Number_of_users_in_dataset_that_are_bots) + len(Number_of_users_in_dataset_that_are_humans)\n",
    "# print(f\"Out of the {total} users in our dataset. {len(Number_of_users_in_dataset_that_are_humans)} are humans, and {len(Number_of_users_in_dataset_that_are_bots)} are bots.\")\n",
    "# print(f\"The dataset consists of {round(100*(len(Number_of_users_in_dataset_that_are_humans)/total),3)}% humans and {round(100*(len(Number_of_users_in_dataset_that_are_bots)/total),3)}% bots respectively.\")\n",
    "# print(f\"399 of our users are missing their statuses_count values. These humans consist of {round(100*(len(list_of_humans)/total),3)}% of our dataset.\")\n",
    "# print(f\"These users will be removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block should explain the the state of the cleaned and generalized dataset. Also stating how balanced it is.\n",
    "\n",
    "From the users cleaning notebook\n",
    ".......................................\n",
    "After cleaning we are left with a \"fairly\" balanced and generalized dataset, that is ready for further use. The dataset contains approx. 45% human and 55% bot users. \n",
    "......................................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv(\"./dataset/tweets_dataset_cleaned.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
