{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main advantages of Hierarchical CLustering:\n",
    "- Do not have to assume any particular number of clusters, any desired number of clusters can be obtained cuttin the endogram\n",
    "- They may correspond to meaningful taxonomies: gerarchical classification of objects\n",
    "\n",
    "## Two main types\n",
    "1) Agglomerative: start with each poin as indivudual clustes and merge clustest while we have k clusters\n",
    "2) Divisive: start with all-inclusive cluster and split while we have k clustes\n",
    "\n",
    "In both cases we have to choose a distance measure and split/merge one cluster at a time, selecting the two clusters that have the minumum distance\n",
    "\n",
    "## Inter-Cluster Distane\n",
    "- MIN (single link): closest points in the two clusters\n",
    "    - take strongly into account the contiguity so it can handle globular shapes but it is sentitive to noise\n",
    "- MAX (complete link): farther points in the two clusters\n",
    "    - less subsceptible to noise but tends to break large clusters and is biased towards globular ones\n",
    "- Group average: mean of all possible distances between the point of the two clusters\n",
    "    - good compromise between single and complete link, still a bit biased toward globular clusters\n",
    "- Distance between centroids: distance between the centroids of the two clusters\n",
    "- Ward's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./dataset/merged_df_dataset_cleaned_with_indicators.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, since we will work on numerical features we remove categorical/nominal? attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"user_id\", \"name\", \"lang\", \"bot\", \"account_created\", \"tweet_id\", \"tweet_created\", \"text\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to normalize the attributes to avoid bias given by the different range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "transformed_array = scaler.fit_transform(df.values)\n",
    "#numpy array of array, each array rappresent a row of the dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
